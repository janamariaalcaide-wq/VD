# -*- coding: utf-8 -*-
"""VD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163osb8WRzi8viOgPJxv5283PmBntEPv9
"""



import streamlit as st
import pandas as pd
import glob
import re
from google.colab import drive
import os
import numpy as np
from sklearn.metrics import (
    precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, log_loss
)

"""# Preprocesado y Análisis"""

drive.mount('/content/drive')

import re
import glob
import os
import pandas as pd

def cargar_archivos(patron, columna_nvariables=True, columna_nfolds=True, columna_fold=True):
    archivos = glob.glob(patron)
    dfs = []
    pattern_nv = re.compile(r'nV(\d+)')
    pattern_nf = re.compile(r'nF(\d+)')
    pattern_s = re.compile(r'_S\d+')  # para eliminar '_S0', '_S1', etc.
    pattern_seed = re.compile(r'Seed[_]?(\d+)')  # para extraer el número de Seed
    pattern_fold = re.compile(r'_Fold(\d+)')  # nuevo patrón para Fold

    for archivo in archivos:
        df = pd.read_csv(archivo)
        nombre_archivo = os.path.basename(archivo)

        # Añadir columna Nvariables
        if columna_nvariables:
            match_nv = pattern_nv.search(archivo)
            if match_nv:
                n_variables = int(match_nv.group(1))
            else:
                n_variables = None
            df['Nvariables'] = n_variables

        # Añadir columna nFolds
        if columna_nfolds:
            match_nf = pattern_nf.search(archivo)
            if match_nf:
                n_folds = int(match_nf.group(1))
            else:
                n_folds = None
            df['nFolds'] = n_folds

        # Añadir columna Fold (si se especifica y si en el nombre aparece)
        if columna_fold:
            match_fold = pattern_fold.search(nombre_archivo)
            if match_fold:
                fold_value = int(match_fold.group(1))
            else:
                fold_value = None
            df['Fold'] = fold_value

        # Si el archivo empieza por 'TestPredCV', eliminar '_S*_'
        if nombre_archivo.startswith('TestPredCV'):
            df.columns = [pattern_s.sub('', col) for col in df.columns]

        # Si en el nombre del archivo aparece 'Seed', extraer el valor
        match_seed = pattern_seed.search(nombre_archivo)
        if 'Seed' in nombre_archivo and match_seed:
            seed_value = int(match_seed.group(1))
            df['Seed'] = seed_value

        dfs.append(df)

    df_total = pd.concat(dfs, ignore_index=True)
    return df_total

patron_testpredcv = "/content/drive/MyDrive/VD/TestPredCV_ID_2C_NvsSD_nR0_nV*_nF*_Seed*.csv"
df_testpredcv_prev = cargar_archivos(patron_testpredcv, columna_nvariables=True,columna_nfolds=True)

patron_leaderboard = "/content/drive/MyDrive/VD/leaderboard_testset_ID_2C_NvsSD_nR0_nV*_nF*_Seed*_Fold*.csv"
df_leaderboard = cargar_archivos(patron_leaderboard, columna_nvariables=True,columna_nfolds=True)

patron_feature_importance = "/content/drive/MyDrive/VD/FeatureImportance_ID_2C_NvsSD_nR0_nV*_nF*.csv"
df_feature_importance = cargar_archivos(patron_feature_importance, columna_nvariables=True,columna_nfolds=True)

patron_metrics = "/content/drive/MyDrive/VD/Metrics_CV_ID_2C_NvsSD_nR0_nV*_nF*_Seed*.csv"
df_metrics = cargar_archivos(patron_metrics, columna_nvariables=True,columna_nfolds=True)

#Pivoto df_testpredcv que tiene los valores para modelos en columnas
def extraer_modelos(df):
    modelos = set()
    for col in df.columns:
        match_numfold = re.match(r'testNumFold_(\w+)', col)
        match_proba = re.match(r'testPredProba_(\w+)', col)
        if match_numfold:
            modelos.add(match_numfold.group(1))
        elif match_proba:
            modelos.add(match_proba.group(1))
    return sorted(modelos)

modelos = extraer_modelos(df_testpredcv_prev)
filas_transformadas = []
for _, fila in df_testpredcv_prev.iterrows():
    base_info = {
        'Nvariables': fila['Nvariables'],
        'nFolds': fila['nFolds'],
        'Seed': fila['Seed'],
        'etiq-id': fila['etiq-id'],
        'clasereal': fila['ED_2Clases']
    }
    for modelo in modelos:
        fila_modelo = base_info.copy()
        fila_modelo['model'] = modelo
        test_numfold_col = f'testNumFold_{modelo}'
        test_proba_col = f'testPredProba_{modelo}'
        fila_modelo['testNumFold'] = fila[test_numfold_col] if test_numfold_col in df_testpredcv_prev.columns else None
        fila_modelo['testPredProba'] = fila[test_proba_col] if test_proba_col in df_testpredcv_prev.columns else None
        filas_transformadas.append(fila_modelo)
df_testpredcv = pd.DataFrame(filas_transformadas)

#voy a ordenar las tablas para poner las claves primero
def reordenar_columnas(df):
    df.columns = [
        'Seed' if col.lower() == 'seed' else col
        for col in df.columns
    ]
    df.columns = [
        'model' if re.search(r'modelname', col, re.IGNORECASE) else col
        for col in df.columns
    ]
    df.columns = [
        'Model' if col.lower() == 'model' else col
        for col in df.columns
    ]
    columnas_prioridad = ['Model', 'Nvariables', 'nFolds', 'Seed', 'Fold']
    columnas_actuales = list(df.columns)
    columnas_prioridad_existentes = [col for col in columnas_prioridad if col in columnas_actuales]
    nuevas_columnas = columnas_prioridad_existentes + [col for col in columnas_actuales if col not in columnas_prioridad_existentes]
    return df[nuevas_columnas]

df_testpredcv = reordenar_columnas(df_testpredcv)
df_leaderboard = reordenar_columnas(df_leaderboard)
df_feature_importance = reordenar_columnas(df_feature_importance)
df_metrics = reordenar_columnas(df_metrics)

df_feature_importance.info()

df_metrics.info()

df_feature_importance.head()

df_metrics.head()

df_testpredcv.head()

df_leaderboard.head()

df_leaderboard.info()

# Creo un diccionario para homogenerizar nombres de df_metrics y df_leader board
mapeo_metricas = {
    # Métricas de precisión
    'precision_macro': 'Precision',
    'Precision_macro': 'Precision',
    'precision': 'Precision',
    'Precision_weighted': 'Precision',
    'Precision_0': 'Precision_0',
    'Precision_1': 'Precision_1',
    # Métricas de recall
    'recall_macro': 'Recall',
    'Recall_macro': 'Recall',
    'recall': 'Recall',
    'Recall_weighted': 'Recall_Weighted',
    'Recall_0': 'Recall_0',
    'Recall_1': 'Recall_1',
    # Métricas de F1
    'f1': 'F1',
    'F1_macro': 'F1_macro',
    'f1_micro': 'F1_micro',
    'F1_weighted': 'F1_weighted',
    # Métricas de evaluación general
    'balanced_accuracy': 'Balanced_Accuracy',
    'Balanced_accuracy': 'Balanced_Accuracy',
    'roc_auc': 'ROC_AUC',
    'Roc_auc': 'ROC_AUC',
    'auc': 'ROC_AUC',
    'pr_auc': 'PR_AUC',
    'average_precision': 'PR_AUC',
    'score_test': 'Score_Test',
    'score_val': 'Score_Validation',
    'log_loss': 'Log_Loss',
}
def renombrar_columnas(df, mapeo):
    columnas_actuales = df.columns
    nuevas_columnas = {}
    for col in columnas_actuales:
        col_lower = col.lower()
        if col in mapeo:
            nuevas_columnas[col] = mapeo[col]
        elif col_lower in [k.lower() for k in mapeo]:
            key = [k for k in mapeo if k.lower() == col_lower][0]
            nuevas_columnas[col] = mapeo[key]
        else:
            nuevas_columnas[col] = col
    df_renombrado = df.rename(columns=nuevas_columnas)
    return df_renombrado
df_metrics = renombrar_columnas(df_metrics, mapeo_metricas)
df_leaderboard = renombrar_columnas(df_leaderboard, mapeo_metricas)

df=df_testpredcv

#Voy a calcular las métricas a partir del detalle del resultado del OOF
resultados_list = []

claves_unicas = df[['Model', 'Nvariables', 'nFolds', 'Seed']].drop_duplicates()

for _, fila_clave in claves_unicas.iterrows():
    filtro = (
        (df['Model'] == fila_clave['Model']) &
        (df['Nvariables'] == fila_clave['Nvariables']) &
        (df['nFolds'] == fila_clave['nFolds']) &
        (df['Seed'] == fila_clave['Seed'])
    )
    df_grupo = df[filtro]

    y_true = df_grupo['clasereal']
    y_proba = df_grupo['testPredProba']
    preds_binarios = (y_proba >= 0.5).astype(int)

    # Verificar si hay solo dos clases para métricas binarias
    clases_unicas_true = np.unique(y_true)
    es_binario = len(clases_unicas_true) == 2

    # Calcular métricas con manejo de errores y diferentes nombres
    resultados = {
        'Model': fila_clave['Model'],
        'Nvariables': fila_clave['Nvariables'],
        'nFolds': fila_clave['nFolds'],
        'Seed': fila_clave['Seed'],
        'Balanced_accuracy': None,
        'Precision_macro': None,
        'Precision_micro': None,
        'Precision_weighted': None,
        'Recall_macro': None,
        'Recall_micro': None,
        'Recall_weighted': None,
        'F1_weighted': None,
        'F1_macro': None,
        'Roc_auc': None,
        'PR_auc': None,
        'Log_loss': None
    }

    try:
        resultados['Balanced_accuracy'] = (preds_binarios == y_true).mean()
    except:
        resultados['Balanced_accuracy'] = np.nan

    if es_binario:
        try:
            resultados['Precision_macro'] = precision_score(y_true, preds_binarios, average='macro', zero_division=0)
            resultados['Recall_macro'] = recall_score(y_true, preds_binarios, average='macro', zero_division=0)
            resultados['F1_macro'] = f1_score(y_true, preds_binarios, zero_division=0, average='macro')
            resultados['Precision_micro'] = precision_score(y_true, preds_binarios, average='micro', zero_division=0)
            resultados['Recall_micro'] = recall_score(y_true, preds_binarios, average='micro', zero_division=0)
            resultados['F1_weighted'] = f1_score(y_true, preds_binarios, average='weighted', zero_division=0)
            resultados['Recall_weighted'] = recall_score(y_true, preds_binarios, average='weighted', zero_division=0)
            resultados['ROC_AUC'] = roc_auc_score(y_true, y_proba)
            resultados['PR_auc'] = average_precision_score(y_true, y_proba)
        except:
            resultados['Precision_macro'] = np.nan
            resultados['Recall_macro'] = np.nan
            resultados['F1_macro'] = np.nan
            resultados['Precision_micro'] = np.nan
            resultados['Recall_micro'] = np.nan
            resultados['F1_weighted'] = np.nan
            resultados['Recall_weighted'] = np.nan
            resultados['ROC_AUC'] = np.nan
            resultados['PR_auc'] = np.nan
    else:
        resultados['ROC_AUC'] = np.nan
        resultados['PR_auc'] = np.nan

    try:
        resultados['Log_loss'] = log_loss(y_true, np.clip(y_proba, 1e-15, 1 - 1e-15))
    except:
        resultados['Log_loss'] = np.nan

    resultados_list.append(resultados)

resultados_df = pd.DataFrame(resultados_list)

print(resultados_df)

ruta = '/content/drive/My Drive/VD/resultados.csv'
# lo persisto para no calcularlo siempre
resultados_df.to_csv(ruta, index=False)

df_oofmetris=resultados_df
df_oofmetris.head()

filtro = (
    (df_metrics['Model'] == 'CatBoost') &
    (df_metrics['Nvariables'] == 8) &
    (df_metrics['nFolds'] == 5) &
    (df_metrics['Seed'] == 0)
)

df_filtrado = df_metrics.loc[filtro]
print(df_filtrado)

df_metrics.head()

"""# Presentación

Voy a acometer objetivos en primer lugar:
- Dadas varias particiones del conjunto de entrenamiento, ¿qué partición tiene un comportamiento medio, cuál tiene el peor comportamiento y cuál el mejor?
- Dados varios modelos de ML que solucionan el problema. ¿Cuáles son los 5 modelos que tienen un comportamiento más robusto?¿cuáles son sus característicsa?
- De todas las variables del dataset de entrada, cuáles son las más relevantes teniendo en cuenta los resultados de cada modelo.
- De todas las configuraciones de parámetros probadas para una misma arquitectura, seleccionar el subconjunto de las más prometedoras.
- ...
"""

st.title("Análisis de comportamiento por partición (nFolds)")
df=df_metrics

nvariables_filter = st.multiselect(
    "Número de variables (Nvariables)",
    options=df['Nvariables'].unique(),
    default=df['Nvariables'].unique()
)

nfolds_filter = st.multiselect(
    "Número de folds (nFolds)",
    options=df['nFolds'].unique(),
    default=df['nFolds'].unique()
)

seed_filter = st.multiselect(
    "Seed",
    options=df['Seed'].unique(),
    default=df['Seed'].unique()
)

# Filtrar el DataFrame según filtros seleccionados
filtered_df = df[
    (df['Nvariables'].isin(nvariables_filter)) &
    (df['nFolds'].isin(nfolds_filter)) &
    (df['Seed'].isin(seed_filter))
]

# Agrupar por nFolds y calcular la media de la métrica que quieres analizar
metric = 'ROC_AUC'  # Puedes cambiarla a otra métrica si quieres
grouped = filtered_df.groupby('nFolds')[metric].mean().reset_index()

# Identificar las particiones con mejor, peor y comportamiento medio
best_fold = grouped.loc[grouped[metric].idxmax()]['nFolds']
worst_fold = grouped.loc[grouped[metric].idxmin()]['nFolds']
medium_value = grouped[metric].mean()

# Clasificar cada partición según su comportamiento
def classify_behavior(row):
    if row['nFolds'] == best_fold:
        return 'Mejor'
    elif row['nFolds'] == worst_fold:
        return 'Peor'
    elif abs(row[metric] - medium_value) < 0.01:  # tolerancia para comportamiento medio
        return 'Medio'
    else:
        return 'Otro'

grouped['Comportamiento'] = grouped.apply(classify_behavior, axis=1)

# Visualización con Altair
chart = alt.Chart(grouped).mark_bar().encode(
    x=alt.X('nFolds:O', title='Número de Folds'),
    y=alt.Y(f'{metric}:Q', title=f'{metric} medio por nFolds'),
    color=alt.Color('Comportamiento:N', title='Comportamiento'),
    tooltip=['nFolds', metric, 'Comportamiento']
).properties(
    title='Comportamiento medio, peor y mejor por partición (nFolds)'
)

st.altair_chart(chart, use_container_width=True)

"""[texto del enlace](https://)# Nueva sección"""